{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we tried to build an LDA model on dataset. We assume the dataset is preprocessed: It was removed stop words and contains only meaningful contents. There are only two columns of the input dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import random\n",
    "import re\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "import gensim # this is LDA package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>abstract</th>\n",
       "      <th>web_url</th>\n",
       "      <th>snippet</th>\n",
       "      <th>lead_paragraph</th>\n",
       "      <th>print_section</th>\n",
       "      <th>print_page</th>\n",
       "      <th>source</th>\n",
       "      <th>multimedia</th>\n",
       "      <th>...</th>\n",
       "      <th>document_type</th>\n",
       "      <th>news_desk</th>\n",
       "      <th>section_name</th>\n",
       "      <th>byline</th>\n",
       "      <th>type_of_material</th>\n",
       "      <th>_id</th>\n",
       "      <th>word_count</th>\n",
       "      <th>uri</th>\n",
       "      <th>subsection_name</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3969</th>\n",
       "      <td>4143</td>\n",
       "      <td>57304</td>\n",
       "      <td>after firing the shots the suspect said “free ...</td>\n",
       "      <td>https://www.nytimes.com/2023/12/07/nyregion/te...</td>\n",
       "      <td>After firing the shots, the suspect said, “Fre...</td>\n",
       "      <td>a man fired a shotgun into the air outside an ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>[{'rank': 0, 'subtype': 'xlarge', 'caption': N...</td>\n",
       "      <td>...</td>\n",
       "      <td>article</td>\n",
       "      <td>Metro</td>\n",
       "      <td>New York</td>\n",
       "      <td>{'original': 'By Grace Ashford and Jay Root', ...</td>\n",
       "      <td>News</td>\n",
       "      <td>nyt://article/250b53b5-1dec-5b00-9629-4cd8dc6a...</td>\n",
       "      <td>764.0</td>\n",
       "      <td>nyt://article/250b53b5-1dec-5b00-9629-4cd8dc6a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-12-07 22:58:03+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>526</td>\n",
       "      <td>53687</td>\n",
       "      <td>the elusive vocalist and producer performed hi...</td>\n",
       "      <td>https://www.nytimes.com/2023/05/04/arts/music/...</td>\n",
       "      <td>The elusive vocalist and producer performed hi...</td>\n",
       "      <td>when a collection of jai paul demos leaked onl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>[{'rank': 0, 'subtype': 'xlarge', 'caption': N...</td>\n",
       "      <td>...</td>\n",
       "      <td>article</td>\n",
       "      <td>Culture</td>\n",
       "      <td>Arts</td>\n",
       "      <td>{'original': '', 'person': [], 'organization':...</td>\n",
       "      <td>News</td>\n",
       "      <td>nyt://article/816d9738-3d2c-5e38-a86b-aa71b1e0...</td>\n",
       "      <td>239.0</td>\n",
       "      <td>nyt://article/816d9738-3d2c-5e38-a86b-aa71b1e0...</td>\n",
       "      <td>Music</td>\n",
       "      <td>2023-05-04 15:24:15+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1948</th>\n",
       "      <td>2051</td>\n",
       "      <td>55212</td>\n",
       "      <td>in two memoirs magazine articles and a times e...</td>\n",
       "      <td>https://www.nytimes.com/2023/05/16/books/amy-s...</td>\n",
       "      <td>In two memoirs, magazine articles and a Times ...</td>\n",
       "      <td>amy silverstein a celebrated writer whose two ...</td>\n",
       "      <td>A</td>\n",
       "      <td>21.0</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>[{'rank': 0, 'subtype': 'xlarge', 'caption': N...</td>\n",
       "      <td>...</td>\n",
       "      <td>article</td>\n",
       "      <td>Obits</td>\n",
       "      <td>Books</td>\n",
       "      <td>{'original': 'By Alex Williams', 'person': [{'...</td>\n",
       "      <td>Obituary (Obit)</td>\n",
       "      <td>nyt://article/89ed7eec-657b-57d7-819f-6e21c1f2...</td>\n",
       "      <td>1176.0</td>\n",
       "      <td>nyt://article/89ed7eec-657b-57d7-819f-6e21c1f2...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-05-16 18:56:39+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2882</th>\n",
       "      <td>3017</td>\n",
       "      <td>56178</td>\n",
       "      <td>the police department will send them up in bro...</td>\n",
       "      <td>https://www.nytimes.com/2023/09/01/nyregion/dr...</td>\n",
       "      <td>The Police Department will send them up in Bro...</td>\n",
       "      <td>the beating of drums the bleating of horns and...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>[{'rank': 0, 'subtype': 'xlarge', 'caption': N...</td>\n",
       "      <td>...</td>\n",
       "      <td>article</td>\n",
       "      <td>Metro</td>\n",
       "      <td>New York</td>\n",
       "      <td>{'original': 'By Claire Fahy', 'person': [{'fi...</td>\n",
       "      <td>News</td>\n",
       "      <td>nyt://article/26d18062-d394-55e2-baea-4632f281...</td>\n",
       "      <td>609.0</td>\n",
       "      <td>nyt://article/26d18062-d394-55e2-baea-4632f281...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-09-01 15:14:54+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2373</th>\n",
       "      <td>2487</td>\n",
       "      <td>55648</td>\n",
       "      <td>addressing an arab league summit in saudi arab...</td>\n",
       "      <td>https://www.nytimes.com/2023/05/19/world/europ...</td>\n",
       "      <td>Addressing an Arab League summit in Saudi Arab...</td>\n",
       "      <td>president volodymyr zelensky of ukraine implor...</td>\n",
       "      <td>A</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>[{'rank': 0, 'subtype': 'xlarge', 'caption': N...</td>\n",
       "      <td>...</td>\n",
       "      <td>article</td>\n",
       "      <td>Foreign</td>\n",
       "      <td>World</td>\n",
       "      <td>{'original': 'By Vivian Nereim', 'person': [{'...</td>\n",
       "      <td>News</td>\n",
       "      <td>nyt://article/7cbd7c63-c42d-5db9-9622-c10432e4...</td>\n",
       "      <td>1263.0</td>\n",
       "      <td>nyt://article/7cbd7c63-c42d-5db9-9622-c10432e4...</td>\n",
       "      <td>Europe</td>\n",
       "      <td>2023-05-19 10:04:31+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5401</th>\n",
       "      <td>5648</td>\n",
       "      <td>58809</td>\n",
       "      <td>“maestro” isn t the first time a supersize sni...</td>\n",
       "      <td>https://www.nytimes.com/2023/12/20/movies/maes...</td>\n",
       "      <td>“Maestro” isn’t the first time a supersize sni...</td>\n",
       "      <td>in august the first trailer for “maestro” a bi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>[{'rank': 0, 'subtype': 'xlarge', 'caption': N...</td>\n",
       "      <td>...</td>\n",
       "      <td>article</td>\n",
       "      <td>Culture</td>\n",
       "      <td>Movies</td>\n",
       "      <td>{'original': 'By Sarah Bahr', 'person': [{'fir...</td>\n",
       "      <td>News</td>\n",
       "      <td>nyt://article/4d1e6b39-9615-57af-974a-6f03c2ff...</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>nyt://article/4d1e6b39-9615-57af-974a-6f03c2ff...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-12-20 10:01:41+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4315</th>\n",
       "      <td>4510</td>\n",
       "      <td>57671</td>\n",
       "      <td>a reader seeks guidance for navigating the ove...</td>\n",
       "      <td>https://www.nytimes.com/2023/12/11/style/susta...</td>\n",
       "      <td>A reader seeks guidance for navigating the ove...</td>\n",
       "      <td>the irony of finding the perfect pair of jeans...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>[{'rank': 0, 'subtype': 'xlarge', 'caption': N...</td>\n",
       "      <td>...</td>\n",
       "      <td>article</td>\n",
       "      <td>Styles</td>\n",
       "      <td>Style</td>\n",
       "      <td>{'original': 'By Vanessa Friedman', 'person': ...</td>\n",
       "      <td>News</td>\n",
       "      <td>nyt://article/e8d53914-fb4e-5f7b-82e9-b2f4b911...</td>\n",
       "      <td>664.0</td>\n",
       "      <td>nyt://article/e8d53914-fb4e-5f7b-82e9-b2f4b911...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-12-11 10:02:38+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4224</th>\n",
       "      <td>4415</td>\n",
       "      <td>57576</td>\n",
       "      <td>her solution a packaging business that sells i...</td>\n",
       "      <td>https://www.nytimes.com/2023/12/10/books/dhoni...</td>\n",
       "      <td>Her solution? A packaging business that sells ...</td>\n",
       "      <td>one evening this fall a crowd of writers and p...</td>\n",
       "      <td>BU</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>[{'rank': 0, 'subtype': 'xlarge', 'caption': N...</td>\n",
       "      <td>...</td>\n",
       "      <td>article</td>\n",
       "      <td>SundayBusiness</td>\n",
       "      <td>Books</td>\n",
       "      <td>{'original': 'By Alexandra Alter', 'person': [...</td>\n",
       "      <td>News</td>\n",
       "      <td>nyt://article/e979bd5a-efa3-54ef-a604-b759f6c9...</td>\n",
       "      <td>3001.0</td>\n",
       "      <td>nyt://article/e979bd5a-efa3-54ef-a604-b759f6c9...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-12-10 10:00:31+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>2125</td>\n",
       "      <td>55286</td>\n",
       "      <td>a worrisome scarcity of cancer drugs has heigh...</td>\n",
       "      <td>https://www.nytimes.com/2023/05/17/health/drug...</td>\n",
       "      <td>A worrisome scarcity of cancer drugs has heigh...</td>\n",
       "      <td>thousands of patients are facing delays in get...</td>\n",
       "      <td>A</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>[{'rank': 0, 'subtype': 'xlarge', 'caption': N...</td>\n",
       "      <td>...</td>\n",
       "      <td>article</td>\n",
       "      <td>Science</td>\n",
       "      <td>Health</td>\n",
       "      <td>{'original': 'By Christina Jewett', 'person': ...</td>\n",
       "      <td>News</td>\n",
       "      <td>nyt://article/a01bcfae-4329-5a7c-94be-92406a08...</td>\n",
       "      <td>1692.0</td>\n",
       "      <td>nyt://article/a01bcfae-4329-5a7c-94be-92406a08...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-05-17 07:00:17+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>1114</td>\n",
       "      <td>54275</td>\n",
       "      <td>if you re at average risk for breast cancer st...</td>\n",
       "      <td>https://www.nytimes.com/2023/05/09/health/mamm...</td>\n",
       "      <td>If you’re at average risk for breast cancer, s...</td>\n",
       "      <td>the us preventive services task force a panel ...</td>\n",
       "      <td>A</td>\n",
       "      <td>21.0</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>[{'rank': 0, 'subtype': 'xlarge', 'caption': N...</td>\n",
       "      <td>...</td>\n",
       "      <td>article</td>\n",
       "      <td>Science</td>\n",
       "      <td>Health</td>\n",
       "      <td>{'original': 'By Roni Caryn Rabin', 'person': ...</td>\n",
       "      <td>News</td>\n",
       "      <td>nyt://article/94e9aea1-caa7-540c-951b-fea87268...</td>\n",
       "      <td>514.0</td>\n",
       "      <td>nyt://article/94e9aea1-caa7-540c-951b-fea87268...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-05-09 15:00:10+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0.1  Unnamed: 0  \\\n",
       "3969          4143       57304   \n",
       "494            526       53687   \n",
       "1948          2051       55212   \n",
       "2882          3017       56178   \n",
       "2373          2487       55648   \n",
       "5401          5648       58809   \n",
       "4315          4510       57671   \n",
       "4224          4415       57576   \n",
       "2020          2125       55286   \n",
       "1053          1114       54275   \n",
       "\n",
       "                                               abstract  \\\n",
       "3969  after firing the shots the suspect said “free ...   \n",
       "494   the elusive vocalist and producer performed hi...   \n",
       "1948  in two memoirs magazine articles and a times e...   \n",
       "2882  the police department will send them up in bro...   \n",
       "2373  addressing an arab league summit in saudi arab...   \n",
       "5401  “maestro” isn t the first time a supersize sni...   \n",
       "4315  a reader seeks guidance for navigating the ove...   \n",
       "4224  her solution a packaging business that sells i...   \n",
       "2020  a worrisome scarcity of cancer drugs has heigh...   \n",
       "1053  if you re at average risk for breast cancer st...   \n",
       "\n",
       "                                                web_url  \\\n",
       "3969  https://www.nytimes.com/2023/12/07/nyregion/te...   \n",
       "494   https://www.nytimes.com/2023/05/04/arts/music/...   \n",
       "1948  https://www.nytimes.com/2023/05/16/books/amy-s...   \n",
       "2882  https://www.nytimes.com/2023/09/01/nyregion/dr...   \n",
       "2373  https://www.nytimes.com/2023/05/19/world/europ...   \n",
       "5401  https://www.nytimes.com/2023/12/20/movies/maes...   \n",
       "4315  https://www.nytimes.com/2023/12/11/style/susta...   \n",
       "4224  https://www.nytimes.com/2023/12/10/books/dhoni...   \n",
       "2020  https://www.nytimes.com/2023/05/17/health/drug...   \n",
       "1053  https://www.nytimes.com/2023/05/09/health/mamm...   \n",
       "\n",
       "                                                snippet  \\\n",
       "3969  After firing the shots, the suspect said, “Fre...   \n",
       "494   The elusive vocalist and producer performed hi...   \n",
       "1948  In two memoirs, magazine articles and a Times ...   \n",
       "2882  The Police Department will send them up in Bro...   \n",
       "2373  Addressing an Arab League summit in Saudi Arab...   \n",
       "5401  “Maestro” isn’t the first time a supersize sni...   \n",
       "4315  A reader seeks guidance for navigating the ove...   \n",
       "4224  Her solution? A packaging business that sells ...   \n",
       "2020  A worrisome scarcity of cancer drugs has heigh...   \n",
       "1053  If you’re at average risk for breast cancer, s...   \n",
       "\n",
       "                                         lead_paragraph print_section  \\\n",
       "3969  a man fired a shotgun into the air outside an ...           NaN   \n",
       "494   when a collection of jai paul demos leaked onl...           NaN   \n",
       "1948  amy silverstein a celebrated writer whose two ...             A   \n",
       "2882  the beating of drums the bleating of horns and...           NaN   \n",
       "2373  president volodymyr zelensky of ukraine implor...             A   \n",
       "5401  in august the first trailer for “maestro” a bi...           NaN   \n",
       "4315  the irony of finding the perfect pair of jeans...           NaN   \n",
       "4224  one evening this fall a crowd of writers and p...            BU   \n",
       "2020  thousands of patients are facing delays in get...             A   \n",
       "1053  the us preventive services task force a panel ...             A   \n",
       "\n",
       "      print_page              source  \\\n",
       "3969         NaN  The New York Times   \n",
       "494          NaN  The New York Times   \n",
       "1948        21.0  The New York Times   \n",
       "2882         NaN  The New York Times   \n",
       "2373         5.0  The New York Times   \n",
       "5401         NaN  The New York Times   \n",
       "4315         NaN  The New York Times   \n",
       "4224         1.0  The New York Times   \n",
       "2020         1.0  The New York Times   \n",
       "1053        21.0  The New York Times   \n",
       "\n",
       "                                             multimedia  ... document_type  \\\n",
       "3969  [{'rank': 0, 'subtype': 'xlarge', 'caption': N...  ...       article   \n",
       "494   [{'rank': 0, 'subtype': 'xlarge', 'caption': N...  ...       article   \n",
       "1948  [{'rank': 0, 'subtype': 'xlarge', 'caption': N...  ...       article   \n",
       "2882  [{'rank': 0, 'subtype': 'xlarge', 'caption': N...  ...       article   \n",
       "2373  [{'rank': 0, 'subtype': 'xlarge', 'caption': N...  ...       article   \n",
       "5401  [{'rank': 0, 'subtype': 'xlarge', 'caption': N...  ...       article   \n",
       "4315  [{'rank': 0, 'subtype': 'xlarge', 'caption': N...  ...       article   \n",
       "4224  [{'rank': 0, 'subtype': 'xlarge', 'caption': N...  ...       article   \n",
       "2020  [{'rank': 0, 'subtype': 'xlarge', 'caption': N...  ...       article   \n",
       "1053  [{'rank': 0, 'subtype': 'xlarge', 'caption': N...  ...       article   \n",
       "\n",
       "           news_desk section_name  \\\n",
       "3969           Metro     New York   \n",
       "494          Culture         Arts   \n",
       "1948           Obits        Books   \n",
       "2882           Metro     New York   \n",
       "2373         Foreign        World   \n",
       "5401         Culture       Movies   \n",
       "4315          Styles        Style   \n",
       "4224  SundayBusiness        Books   \n",
       "2020         Science       Health   \n",
       "1053         Science       Health   \n",
       "\n",
       "                                                 byline type_of_material  \\\n",
       "3969  {'original': 'By Grace Ashford and Jay Root', ...             News   \n",
       "494   {'original': '', 'person': [], 'organization':...             News   \n",
       "1948  {'original': 'By Alex Williams', 'person': [{'...  Obituary (Obit)   \n",
       "2882  {'original': 'By Claire Fahy', 'person': [{'fi...             News   \n",
       "2373  {'original': 'By Vivian Nereim', 'person': [{'...             News   \n",
       "5401  {'original': 'By Sarah Bahr', 'person': [{'fir...             News   \n",
       "4315  {'original': 'By Vanessa Friedman', 'person': ...             News   \n",
       "4224  {'original': 'By Alexandra Alter', 'person': [...             News   \n",
       "2020  {'original': 'By Christina Jewett', 'person': ...             News   \n",
       "1053  {'original': 'By Roni Caryn Rabin', 'person': ...             News   \n",
       "\n",
       "                                                    _id word_count  \\\n",
       "3969  nyt://article/250b53b5-1dec-5b00-9629-4cd8dc6a...      764.0   \n",
       "494   nyt://article/816d9738-3d2c-5e38-a86b-aa71b1e0...      239.0   \n",
       "1948  nyt://article/89ed7eec-657b-57d7-819f-6e21c1f2...     1176.0   \n",
       "2882  nyt://article/26d18062-d394-55e2-baea-4632f281...      609.0   \n",
       "2373  nyt://article/7cbd7c63-c42d-5db9-9622-c10432e4...     1263.0   \n",
       "5401  nyt://article/4d1e6b39-9615-57af-974a-6f03c2ff...     1020.0   \n",
       "4315  nyt://article/e8d53914-fb4e-5f7b-82e9-b2f4b911...      664.0   \n",
       "4224  nyt://article/e979bd5a-efa3-54ef-a604-b759f6c9...     3001.0   \n",
       "2020  nyt://article/a01bcfae-4329-5a7c-94be-92406a08...     1692.0   \n",
       "1053  nyt://article/94e9aea1-caa7-540c-951b-fea87268...      514.0   \n",
       "\n",
       "                                                    uri subsection_name  \\\n",
       "3969  nyt://article/250b53b5-1dec-5b00-9629-4cd8dc6a...             NaN   \n",
       "494   nyt://article/816d9738-3d2c-5e38-a86b-aa71b1e0...           Music   \n",
       "1948  nyt://article/89ed7eec-657b-57d7-819f-6e21c1f2...             NaN   \n",
       "2882  nyt://article/26d18062-d394-55e2-baea-4632f281...             NaN   \n",
       "2373  nyt://article/7cbd7c63-c42d-5db9-9622-c10432e4...          Europe   \n",
       "5401  nyt://article/4d1e6b39-9615-57af-974a-6f03c2ff...             NaN   \n",
       "4315  nyt://article/e8d53914-fb4e-5f7b-82e9-b2f4b911...             NaN   \n",
       "4224  nyt://article/e979bd5a-efa3-54ef-a604-b759f6c9...             NaN   \n",
       "2020  nyt://article/a01bcfae-4329-5a7c-94be-92406a08...             NaN   \n",
       "1053  nyt://article/94e9aea1-caa7-540c-951b-fea87268...             NaN   \n",
       "\n",
       "                           date  \n",
       "3969  2023-12-07 22:58:03+00:00  \n",
       "494   2023-05-04 15:24:15+00:00  \n",
       "1948  2023-05-16 18:56:39+00:00  \n",
       "2882  2023-09-01 15:14:54+00:00  \n",
       "2373  2023-05-19 10:04:31+00:00  \n",
       "5401  2023-12-20 10:01:41+00:00  \n",
       "4315  2023-12-11 10:02:38+00:00  \n",
       "4224  2023-12-10 10:00:31+00:00  \n",
       "2020  2023-05-17 07:00:17+00:00  \n",
       "1053  2023-05-09 15:00:10+00:00  \n",
       "\n",
       "[10 rows x 23 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This block read dataset and look at its sample\n",
    "\n",
    "df = pd.read_csv('nyt.csv')\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2092    president biden selected shalanda young and st...\n",
       "2982    lisa donovan weaves her own story into this re...\n",
       "5086    the tree which has been decorated each year be...\n",
       "5655    tammy murphy is backed by some new jersey demo...\n",
       "5620      plus how wealthy was the family in “home alone”\n",
       "4597    we have the tools we need to create a world wh...\n",
       "2355    seven brand new and time tested books about sl...\n",
       "5603    a huge ice sheet appears to have melted about ...\n",
       "889     everyone gets mad about spoilers but science s...\n",
       "433     a decorated writer in the s nancy hale analyze...\n",
       "Name: abstract, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For the test purpose, we use only abstract here.\n",
    "\n",
    "X = df.abstract\n",
    "\n",
    "X.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Further Data preparing\n",
    "\n",
    "The book articles are removed from the initial list of book properties\n",
    "Punctuation is removed\n",
    "Article strings are turned into tokens, and stop words are removed\n",
    "Numeric tokens were removed\n",
    "Bigrams are created\n",
    "Words are lemmatized to reduce words of similar roots to similar tokens\n",
    "English first names are removed, as these are often very prevalent in articles (note that bigrams of full names are retained)\n",
    "Token amounts per article are rounded off at an exponentially increasing rate (we'll more explain when we get there)\n",
    "Tokens of that are less than four characters are removed\n",
    "Only tokens that appear in at least ten articles are kept\n",
    "Those tokens that appear in more than 15% of all articles will be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val = train_test_split(X, test_size=0.2, shuffle= True, random_state= 216)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from the training data using a Count vectorizer\n"
     ]
    }
   ],
   "source": [
    "# We start by tokenize the texts\n",
    "\n",
    "print(\"Extracting features from the training data using a Count vectorizer\")\n",
    "\n",
    "vectorizer_count = CountVectorizer(#sublinear_tf=True, \n",
    "                                   max_df=0.15, #remove words that have a document frequency strictly higher than 15%\n",
    "                                   min_df=0.001, #remove words that have a document frequency strictly lower than 0.1%\n",
    "                                   stop_words='english', #removing stop words\n",
    "                                   ngram_range=(1,2), #unigrams and bigrams are generated\n",
    "                                   max_features=2**14)\n",
    "\n",
    "#We train the vectorizer using X_train\n",
    "X_train_count = vectorizer_count.fit_transform(X_train)\n",
    "X_val_count = vectorizer_count.transform(X_val) # Extracting features from the test data using the same vectorizer\n",
    "#X_test_tfidf = vectorizer_tfidf.fit_transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from the training data using a TF-IDF vectorizer\n",
      "0.004796623177283192\n",
      "0.004604758250191865\n"
     ]
    }
   ],
   "source": [
    "# We start by tokenize the texts\n",
    "\n",
    "print(\"Extracting features from the training data using a TF-IDF vectorizer\")\n",
    "\n",
    "vectorizer_tfidf = TfidfVectorizer(sublinear_tf=True, \n",
    "                                   max_df=0.15, #remove words that have a document frequency strictly higher than 15%\n",
    "                                   min_df=0.001, #remove words that have a document frequency strictly lower than 0.1%\n",
    "                                   stop_words='english', #removing stop words\n",
    "                                   ngram_range=(1,2), #unigrams and bigrams are generated\n",
    "                                   max_features=2**14)\n",
    "\n",
    "#We train the vectorizer using X_train\n",
    "X_train_tfidf = vectorizer_tfidf.fit_transform(X_train)\n",
    "X_val_tfidf = vectorizer_tfidf.transform(X_val) # Extracting features from the test data using the same vectorizer\n",
    "#X_test_tfidf = vectorizer_tfidf.fit_transform(X_test)\n",
    "\n",
    "# To visualize bow we transform them into a dataframe\n",
    "X_train_tok = pd.DataFrame(X_train_tfidf.toarray(), index = X_train, columns= vectorizer_tfidf.get_feature_names_out())\n",
    "X_val_tok = pd.DataFrame(X_val_tfidf.toarray(), index = X_val, columns= vectorizer_tfidf.get_feature_names_out())\n",
    "\n",
    "X_train_tok_empty = X_train_tok.loc[(X_train_tok== 0).all(axis= 1)]\n",
    "X_val_tok_empty = X_val_tok.loc[(X_val_tok== 0).all(axis= 1)]\n",
    "\n",
    "print(len(X_train_tok_empty)/len(X_train_tok))\n",
    "\n",
    "print(len(X_val_tok_empty)/len(X_val_tok))\n",
    "\n",
    "# There are some empty tokenizations, we should definitely look into them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>abortion</th>\n",
       "      <th>abroad</th>\n",
       "      <th>abuse</th>\n",
       "      <th>access</th>\n",
       "      <th>according</th>\n",
       "      <th>according new</th>\n",
       "      <th>account</th>\n",
       "      <th>accounts</th>\n",
       "      <th>...</th>\n",
       "      <th>york</th>\n",
       "      <th>york city</th>\n",
       "      <th>york times</th>\n",
       "      <th>yorkers</th>\n",
       "      <th>young</th>\n",
       "      <th>young people</th>\n",
       "      <th>younger</th>\n",
       "      <th>youth</th>\n",
       "      <th>zelensky</th>\n",
       "      <th>zelensky ukraine</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abstract</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>we have more incentives to be heterodox</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>also soothing and satisfying crisp gnocchi with sausage and peas soy sauce noodles with cabbage and fried eggs</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asher seems to be losing his grip except when what in his grip is a bunch of nails</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>solving brad wiegmann crossword requires prudence</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and supersize your almond croissant</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>four insiders on where to go for rooftop drinks treasure hunting and more</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 2591 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    ability  able  abortion  \\\n",
       "abstract                                                                      \n",
       "we have more incentives to be heterodox                 0.0   0.0       0.0   \n",
       "also soothing and satisfying crisp gnocchi with...      0.0   0.0       0.0   \n",
       "asher seems to be losing his grip except when w...      0.0   0.0       0.0   \n",
       "solving brad wiegmann crossword requires prudence       0.0   0.0       0.0   \n",
       "and supersize your almond croissant                     0.0   0.0       0.0   \n",
       "four insiders on where to go for rooftop drinks...      0.0   0.0       0.0   \n",
       "\n",
       "                                                    abroad  abuse  access  \\\n",
       "abstract                                                                    \n",
       "we have more incentives to be heterodox                0.0    0.0     0.0   \n",
       "also soothing and satisfying crisp gnocchi with...     0.0    0.0     0.0   \n",
       "asher seems to be losing his grip except when w...     0.0    0.0     0.0   \n",
       "solving brad wiegmann crossword requires prudence      0.0    0.0     0.0   \n",
       "and supersize your almond croissant                    0.0    0.0     0.0   \n",
       "four insiders on where to go for rooftop drinks...     0.0    0.0     0.0   \n",
       "\n",
       "                                                    according  according new  \\\n",
       "abstract                                                                       \n",
       "we have more incentives to be heterodox                   0.0            0.0   \n",
       "also soothing and satisfying crisp gnocchi with...        0.0            0.0   \n",
       "asher seems to be losing his grip except when w...        0.0            0.0   \n",
       "solving brad wiegmann crossword requires prudence         0.0            0.0   \n",
       "and supersize your almond croissant                       0.0            0.0   \n",
       "four insiders on where to go for rooftop drinks...        0.0            0.0   \n",
       "\n",
       "                                                    account  accounts  ...  \\\n",
       "abstract                                                               ...   \n",
       "we have more incentives to be heterodox                 0.0       0.0  ...   \n",
       "also soothing and satisfying crisp gnocchi with...      0.0       0.0  ...   \n",
       "asher seems to be losing his grip except when w...      0.0       0.0  ...   \n",
       "solving brad wiegmann crossword requires prudence       0.0       0.0  ...   \n",
       "and supersize your almond croissant                     0.0       0.0  ...   \n",
       "four insiders on where to go for rooftop drinks...      0.0       0.0  ...   \n",
       "\n",
       "                                                    york  york city  \\\n",
       "abstract                                                              \n",
       "we have more incentives to be heterodox              0.0        0.0   \n",
       "also soothing and satisfying crisp gnocchi with...   0.0        0.0   \n",
       "asher seems to be losing his grip except when w...   0.0        0.0   \n",
       "solving brad wiegmann crossword requires prudence    0.0        0.0   \n",
       "and supersize your almond croissant                  0.0        0.0   \n",
       "four insiders on where to go for rooftop drinks...   0.0        0.0   \n",
       "\n",
       "                                                    york times  yorkers  \\\n",
       "abstract                                                                  \n",
       "we have more incentives to be heterodox                    0.0      0.0   \n",
       "also soothing and satisfying crisp gnocchi with...         0.0      0.0   \n",
       "asher seems to be losing his grip except when w...         0.0      0.0   \n",
       "solving brad wiegmann crossword requires prudence          0.0      0.0   \n",
       "and supersize your almond croissant                        0.0      0.0   \n",
       "four insiders on where to go for rooftop drinks...         0.0      0.0   \n",
       "\n",
       "                                                    young  young people  \\\n",
       "abstract                                                                  \n",
       "we have more incentives to be heterodox               0.0           0.0   \n",
       "also soothing and satisfying crisp gnocchi with...    0.0           0.0   \n",
       "asher seems to be losing his grip except when w...    0.0           0.0   \n",
       "solving brad wiegmann crossword requires prudence     0.0           0.0   \n",
       "and supersize your almond croissant                   0.0           0.0   \n",
       "four insiders on where to go for rooftop drinks...    0.0           0.0   \n",
       "\n",
       "                                                    younger  youth  zelensky  \\\n",
       "abstract                                                                       \n",
       "we have more incentives to be heterodox                 0.0    0.0       0.0   \n",
       "also soothing and satisfying crisp gnocchi with...      0.0    0.0       0.0   \n",
       "asher seems to be losing his grip except when w...      0.0    0.0       0.0   \n",
       "solving brad wiegmann crossword requires prudence       0.0    0.0       0.0   \n",
       "and supersize your almond croissant                     0.0    0.0       0.0   \n",
       "four insiders on where to go for rooftop drinks...      0.0    0.0       0.0   \n",
       "\n",
       "                                                    zelensky ukraine  \n",
       "abstract                                                              \n",
       "we have more incentives to be heterodox                          0.0  \n",
       "also soothing and satisfying crisp gnocchi with...               0.0  \n",
       "asher seems to be losing his grip except when w...               0.0  \n",
       "solving brad wiegmann crossword requires prudence                0.0  \n",
       "and supersize your almond croissant                              0.0  \n",
       "four insiders on where to go for rooftop drinks...               0.0  \n",
       "\n",
       "[6 rows x 2591 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_tok_empty\n",
    "# So it seems like we have some non-English articles (ruled out by min_df) and also some short and ambiguous abstratc. \n",
    "# For now we will rule them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2591\n",
      "2591\n"
     ]
    }
   ],
   "source": [
    "# Look at unique tokens and articles\n",
    "print(len(vectorizer_tfidf.get_feature_names_out()))\n",
    "\n",
    "print(len(vectorizer_count.get_feature_names_out()))\n",
    "\n",
    "#they are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at maximal numbers of tokens\n",
    "np.max(np.count_nonzero(X_train_count.toarray(),axis=1))\n",
    "#np.min(np.count_nonzero(X_val_tfidf,axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model prep\n",
    "In this part we create our dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a dictionary for our bow\n",
    "\n",
    "dictionary_tfidf = vectorizer_tfidf.get_feature_names_out()\n",
    "dictionary_count = vectorizer_count.get_feature_names_out()\n",
    "\n",
    "len(dictionary_count[np.invert(np.in1d(dictionary_count,dictionary_tfidf))])\n",
    "\n",
    "# this line shows that the two dictionary are exactly the same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['films' 'original' 'played' 'sports' 'tale' 'wasn' 'winning']\n"
     ]
    }
   ],
   "source": [
    "def tok_report(bow, num):\n",
    "    article = bow[num] \n",
    "    arg = article.nonzero()\n",
    "    print(dictionary[arg])\n",
    "\n",
    "tok_report(X_val_tfidf, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gensim.models import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse._csr.csr_matrix"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[206], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m lda \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mLdaModel(X_train_count, \u001b[38;5;66;03m#the bow of corpus\u001b[39;00m\n\u001b[0;32m      2\u001b[0m                       num_topics\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\gensim\\models\\ldamodel.py:440\u001b[0m, in \u001b[0;36mLdaModel.__init__\u001b[1;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid2word \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    439\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno word id mapping provided; initializing from corpus, assuming identity\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 440\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid2word \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mdict_from_corpus(corpus)\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_terms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid2word)\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid2word) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\gensim\\utils.py:900\u001b[0m, in \u001b[0;36mdict_from_corpus\u001b[1;34m(corpus)\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdict_from_corpus\u001b[39m(corpus):\n\u001b[0;32m    880\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Scan corpus for all word ids that appear in it, then construct a mapping\u001b[39;00m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;124;03m    which maps each `word_id` -> `str(word_id)`.\u001b[39;00m\n\u001b[0;32m    882\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    898\u001b[0m \n\u001b[0;32m    899\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 900\u001b[0m     num_terms \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m get_max_id(corpus)\n\u001b[0;32m    901\u001b[0m     id2word \u001b[38;5;241m=\u001b[39m FakeDict(num_terms)\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m id2word\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\gensim\\utils.py:807\u001b[0m, in \u001b[0;36mget_max_id\u001b[1;34m(corpus)\u001b[0m\n\u001b[0;32m    805\u001b[0m maxid \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m document \u001b[38;5;129;01min\u001b[39;00m corpus:\n\u001b[1;32m--> 807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m document:\n\u001b[0;32m    808\u001b[0m         maxid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(maxid, \u001b[38;5;28mmax\u001b[39m(fieldid \u001b[38;5;28;01mfor\u001b[39;00m fieldid, _ \u001b[38;5;129;01min\u001b[39;00m document))\n\u001b[0;32m    809\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maxid\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_base.py:332\u001b[0m, in \u001b[0;36m_spbase.__bool__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnnz \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 332\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe truth value of an array with more than one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    333\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melement is ambiguous. Use a.any() or a.all().\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "lda = models.LdaModel(X_train_count, #the bow of corpus\n",
    "                      num_topics=4) # We should do cv to optimize this value \n",
    "                                  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code has been modified from Craig and Ping's work.\n",
    "References: https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#3importpackages, https://markroxor.github.io/gensim/static/notebooks/lda_training_tips.html, and https://radimrehurek.com/gensim/models/ldamodel.html.\n",
    "\n",
    "New code is from the first link so we can modify it later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import json\n",
    "import os\n",
    "#os.environ['KMP_WARNINGS'] = '0'\n",
    "#os.environ['OMP_NUM_THREADS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "df = pd.read_csv('data/nyt_metadata.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that are not needed\n",
    "columns_to_drop = ['web_url', \n",
    "                   'snippet', \n",
    "                   'lead_paragraph', \n",
    "                   'print_section', \n",
    "                   'print_page', \n",
    "                   'source', \n",
    "                   'multimedia', \n",
    "                   'news_desk',\n",
    "                   'byline',\n",
    "                   '_id',\n",
    "                   'uri',\n",
    "                   'subsection_name',\n",
    "                   'word_count',\n",
    "                   'keywords']\n",
    "df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Drop rows with missing abstracts\n",
    "drop_rows = df[df['abstract'].isnull()].index\n",
    "df.drop(drop_rows, inplace=True)\n",
    "\n",
    "# Change the date column to datetime\n",
    "df['pub_date'] = pd.to_datetime(df['pub_date'])\n",
    "\n",
    "# Change the abstract column to string\n",
    "df['abstract'] = df['abstract'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the 'main' value from JSON-like strings\n",
    "def extract_main(headline_str):\n",
    "    try:\n",
    "        # Safely evaluate the string to convert it to a dictionary\n",
    "        json_dict = ast.literal_eval(headline_str)\n",
    "        # Access and return the 'main' key\n",
    "        return json_dict.get('main', None)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return None\n",
    "\n",
    "# Apply the function to the 'headline' column\n",
    "df['headline'] = df['headline'].apply(extract_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>abstract</th>\n",
       "      <th>headline</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>document_type</th>\n",
       "      <th>section_name</th>\n",
       "      <th>type_of_material</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1813</td>\n",
       "      <td>Economic hardship, climate change, political i...</td>\n",
       "      <td>Title 42 Is Gone, but Not the Conditions Drivi...</td>\n",
       "      <td>2023-05-15 01:24:42+00:00</td>\n",
       "      <td>article</td>\n",
       "      <td>U.S.</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1814</td>\n",
       "      <td>It’s election night in America. Stay away from...</td>\n",
       "      <td>‘Succession’ Season 4, Episode 8 Recap: The Wi...</td>\n",
       "      <td>2023-05-15 02:01:05+00:00</td>\n",
       "      <td>article</td>\n",
       "      <td>Arts</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1815</td>\n",
       "      <td>Tom is stressed in dress shoes, Shiv hides ben...</td>\n",
       "      <td>‘Succession’ Style, Episode 8: Some People Jus...</td>\n",
       "      <td>2023-05-15 02:15:04+00:00</td>\n",
       "      <td>article</td>\n",
       "      <td>Style</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1816</td>\n",
       "      <td>No corrections appeared in print on Monday, Ma...</td>\n",
       "      <td>No Corrections: May 15, 2023</td>\n",
       "      <td>2023-05-15 03:55:48+00:00</td>\n",
       "      <td>article</td>\n",
       "      <td>Corrections</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1817</td>\n",
       "      <td>Quotation of the Day for Monday, May 15, 2023.</td>\n",
       "      <td>Quotation of the Day: When Your Champions Leag...</td>\n",
       "      <td>2023-05-15 03:55:57+00:00</td>\n",
       "      <td>article</td>\n",
       "      <td>Corrections</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           abstract  \\\n",
       "0        1813  Economic hardship, climate change, political i...   \n",
       "1        1814  It’s election night in America. Stay away from...   \n",
       "2        1815  Tom is stressed in dress shoes, Shiv hides ben...   \n",
       "3        1816  No corrections appeared in print on Monday, Ma...   \n",
       "4        1817     Quotation of the Day for Monday, May 15, 2023.   \n",
       "\n",
       "                                            headline  \\\n",
       "0  Title 42 Is Gone, but Not the Conditions Drivi...   \n",
       "1  ‘Succession’ Season 4, Episode 8 Recap: The Wi...   \n",
       "2  ‘Succession’ Style, Episode 8: Some People Jus...   \n",
       "3                       No Corrections: May 15, 2023   \n",
       "4  Quotation of the Day: When Your Champions Leag...   \n",
       "\n",
       "                   pub_date document_type section_name type_of_material  \n",
       "0 2023-05-15 01:24:42+00:00       article         U.S.             News  \n",
       "1 2023-05-15 02:01:05+00:00       article         Arts             News  \n",
       "2 2023-05-15 02:15:04+00:00       article        Style             News  \n",
       "3 2023-05-15 03:55:48+00:00       article  Corrections             News  \n",
       "4 2023-05-15 03:55:57+00:00       article  Corrections             News  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 2000 of the abstracts for the analysis\n",
    "abstracts = df['abstract'][:2000]\n",
    "docs = abstracts.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the documents.\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Split the documents into tokens.\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for idx in range(len(docs)):\n",
    "    docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "# Remove numbers, but not words that contain numbers.\n",
    "docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "\n",
    "# Remove words that are only one character.\n",
    "docs = [[token for token in doc if len(token) > 1] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['economic',\n",
       "  'hardship',\n",
       "  'climate',\n",
       "  'change',\n",
       "  'political',\n",
       "  'instability',\n",
       "  'and',\n",
       "  'gang',\n",
       "  'violence',\n",
       "  'will',\n",
       "  'continue',\n",
       "  'to',\n",
       "  'spur',\n",
       "  'emigration',\n",
       "  'from',\n",
       "  'many',\n",
       "  'corners',\n",
       "  'of',\n",
       "  'the',\n",
       "  'world'],\n",
       " ['it',\n",
       "  'election',\n",
       "  'night',\n",
       "  'in',\n",
       "  'america',\n",
       "  'stay',\n",
       "  'away',\n",
       "  'from',\n",
       "  'the',\n",
       "  'bodega',\n",
       "  'sushi']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/schinella/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize the documents.\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4\u001b[0m\n\n  Searched in:\n    - '/Users/schinella/nltk_data'\n    - '/Users/schinella/opt/anaconda3/nltk_data'\n    - '/Users/schinella/opt/anaconda3/share/nltk_data'\n    - '/Users/schinella/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4.zip/omw-1.4/\u001b[0m\n\n  Searched in:\n    - '/Users/schinella/nltk_data'\n    - '/Users/schinella/opt/anaconda3/nltk_data'\n    - '/Users/schinella/opt/anaconda3/share/nltk_data'\n    - '/Users/schinella/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Lemmatize all words in documents.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[0;32m----> 3\u001b[0m docs \u001b[38;5;241m=\u001b[39m [[lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(token) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc] \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Lemmatize all words in documents.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[0;32m----> 3\u001b[0m docs \u001b[38;5;241m=\u001b[39m [[lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(token) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc] \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Lemmatize all words in documents.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[0;32m----> 3\u001b[0m docs \u001b[38;5;241m=\u001b[39m [[\u001b[43mlemmatizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc] \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/stem/wordnet.py:45\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize\u001b[39m(\u001b[38;5;28mself\u001b[39m, word: \u001b[38;5;28mstr\u001b[39m, pos: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;124;03m\"\"\"Lemmatize `word` using WordNet's built-in morphy function.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m    Returns the input word unchanged if it cannot be found in WordNet.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m    :return: The lemma of `word`, for the given `pos`.\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m     lemmas \u001b[38;5;241m=\u001b[39m \u001b[43mwn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_morphy\u001b[49m(word, pos)\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(lemmas, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m lemmas \u001b[38;5;28;01melse\u001b[39;00m word\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/corpus/util.py:89\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__reader_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# This is where the magic happens!  Transform ourselves into\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# the corpus by modifying our own __dict__ and __class__ to\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# match that of the corpus.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:1176\u001b[0m, in \u001b[0;36mWordNetCorpusReader.__init__\u001b[0;34m(self, root, omw_reader)\u001b[0m\n\u001b[1;32m   1172\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe multilingual functions are not available with this Wordnet version\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1174\u001b[0m     )\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprovenances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43momw_prov\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;66;03m# A cache to store the wordnet data of multiple languages\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang_data \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mlist\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:1285\u001b[0m, in \u001b[0;36mWordNetCorpusReader.omw_prov\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1283\u001b[0m provdict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1284\u001b[0m provdict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1285\u001b[0m fileids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_omw_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileids\u001b[49m()\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fileid \u001b[38;5;129;01min\u001b[39;00m fileids:\n\u001b[1;32m   1287\u001b[0m     prov, langfile \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplit(fileid)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4\u001b[0m\n\n  Searched in:\n    - '/Users/schinella/nltk_data'\n    - '/Users/schinella/opt/anaconda3/nltk_data'\n    - '/Users/schinella/opt/anaconda3/share/nltk_data'\n    - '/Users/schinella/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize all words in documents.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute bigrams.\n",
    "\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['economic',\n",
       "  'hardship',\n",
       "  'climate',\n",
       "  'change',\n",
       "  'political',\n",
       "  'instability',\n",
       "  'and',\n",
       "  'gang',\n",
       "  'violence',\n",
       "  'will',\n",
       "  'continue',\n",
       "  'to',\n",
       "  'spur',\n",
       "  'emigration',\n",
       "  'from',\n",
       "  'many',\n",
       "  'corners',\n",
       "  'of',\n",
       "  'the',\n",
       "  'world']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rare and common tokens.\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it',\n",
       " 'election',\n",
       " 'night',\n",
       " 'in',\n",
       " 'america',\n",
       " 'stay',\n",
       " 'away',\n",
       " 'from',\n",
       " 'the',\n",
       " 'bodega',\n",
       " 'sushi']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/schinella/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define the stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stopwords from each document\n",
    "filtered_docs = [[word for word in doc if word not in stop_words] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['economic',\n",
       "  'hardship',\n",
       "  'climate',\n",
       "  'change',\n",
       "  'political',\n",
       "  'instability',\n",
       "  'gang',\n",
       "  'violence',\n",
       "  'continue',\n",
       "  'spur',\n",
       "  'emigration',\n",
       "  'many',\n",
       "  'corners',\n",
       "  'world']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_docs[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concatenate each entry into a single string after removing stopwords\n",
    "#cleaned_docs = [' '.join(doc) for doc in filtered_docs]\n",
    "\n",
    "## Example of how to print the filtered and concatenated result for the first document\n",
    "#print(cleaned_docs[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize data.\n",
    "\n",
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)],\n",
       " [(2, 1), (9, 1), (10, 1), (11, 1)]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(filtered_docs)\n",
    "\n",
    "# Create Corpus\n",
    "texts = filtered_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('change', 1),\n",
       "  ('climate', 1),\n",
       "  ('continue', 1),\n",
       "  ('corners', 1),\n",
       "  ('economic', 1),\n",
       "  ('emigration', 1),\n",
       "  ('gang', 1),\n",
       "  ('hardship', 1),\n",
       "  ('instability', 1)]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.120*\"election\" + 0.108*\"giants\" + 0.101*\"days\" + 0.042*\"citizens\" + '\n",
      "  '0.032*\"beneath\" + 0.000*\"bottom\" + 0.000*\"workers\" + 0.000*\"monrovia\" + '\n",
      "  '0.000*\"nonpartisan\" + 0.000*\"things\"'),\n",
      " (1,\n",
      "  '0.062*\"ability\" + 0.000*\"tones\" + 0.000*\"drab\" + 0.000*\"earth\" + '\n",
      "  '0.000*\"house\" + 0.000*\"things\" + 0.000*\"inspire\" + 0.000*\"question\" + '\n",
      "  '0.000*\"races\" + 0.000*\"recent\"'),\n",
      " (2,\n",
      "  '0.091*\"coming\" + 0.065*\"believed\" + 0.044*\"protect\" + 0.042*\"battle\" + '\n",
      "  '0.035*\"liquefied\" + 0.031*\"overseas\" + 0.000*\"country\" + 0.000*\"number\" + '\n",
      "  '0.000*\"spruce\" + 0.000*\"reader\"'),\n",
      " (3,\n",
      "  '0.079*\"bill\" + 0.044*\"nation\" + 0.039*\"ending\" + 0.039*\"governor\" + '\n",
      "  '0.036*\"democratic\" + 0.000*\"nonpartisan\" + 0.000*\"monrovia\" + '\n",
      "  '0.000*\"storybook\" + 0.000*\"country\" + 0.000*\"northwest\"'),\n",
      " (4,\n",
      "  '0.059*\"several\" + 0.055*\"mostly\" + 0.041*\"showtime\" + 0.040*\"adjust\" + '\n",
      "  '0.034*\"word\" + 0.000*\"bottom\" + 0.000*\"study\" + 0.000*\"meant\" + '\n",
      "  '0.000*\"country\" + 0.000*\"important\"'),\n",
      " (5,\n",
      "  '0.110*\"committed\" + 0.075*\"exporter\" + 0.067*\"has_been\" + 0.049*\"tobago\" + '\n",
      "  '0.032*\"layers\" + 0.031*\"ban\" + 0.029*\"realities\" + 0.000*\"workers\" + '\n",
      "  '0.000*\"tries\" + 0.000*\"nonpartisan\"'),\n",
      " (6,\n",
      "  '0.135*\"spur\" + 0.125*\"violence\" + 0.095*\"publishing\" + 0.079*\"hopeful\" + '\n",
      "  '0.069*\"reflect\" + 0.052*\"china\" + 0.037*\"manhunt\" + 0.032*\"efforts\" + '\n",
      "  '0.025*\"gas\" + 0.019*\"remains\"'),\n",
      " (7,\n",
      "  '0.081*\"diplomatic\" + 0.081*\"brings\" + 0.066*\"corners\" + 0.000*\"bottom\" + '\n",
      "  '0.000*\"country\" + 0.000*\"across\" + 0.000*\"tries\" + 0.000*\"cottage\" + '\n",
      "  '0.000*\"drab\" + 0.000*\"monrovia\"'),\n",
      " (8,\n",
      "  '0.095*\"lady\" + 0.048*\"warning\" + 0.043*\"shoes\" + 0.036*\"pressure\" + '\n",
      "  '0.030*\"old\" + 0.028*\"ruined\" + 0.000*\"tones\" + 0.000*\"important\" + '\n",
      "  '0.000*\"bedroom\" + 0.000*\"laws\"'),\n",
      " (9,\n",
      "  '0.070*\"extreme\" + 0.000*\"tones\" + 0.000*\"nonpartisan\" + 0.000*\"bedroom\" + '\n",
      "  '0.000*\"question\" + 0.000*\"number\" + 0.000*\"reader\" + 0.000*\"spruce\" + '\n",
      "  '0.000*\"building\" + 0.000*\"cottage\"'),\n",
      " (10,\n",
      "  '0.100*\"economic\" + 0.091*\"gang\" + 0.087*\"political\" + 0.078*\"change\" + '\n",
      "  '0.038*\"dress\" + 0.032*\"sushi\" + 0.029*\"appeared_in\" + 0.029*\"america\" + '\n",
      "  '0.024*\"corrections\" + 0.023*\"willa\"'),\n",
      " (11,\n",
      "  '0.069*\"thousands\" + 0.051*\"tens\" + 0.000*\"meant\" + 0.000*\"country\" + '\n",
      "  '0.000*\"drab\" + 0.000*\"tries\" + 0.000*\"places\" + 0.000*\"things\" + '\n",
      "  '0.000*\"incite\" + 0.000*\"tones\"'),\n",
      " (12,\n",
      "  '0.077*\"made\" + 0.029*\"leaders\" + 0.000*\"workers\" + 0.000*\"bottom\" + '\n",
      "  '0.000*\"tries\" + 0.000*\"meant\" + 0.000*\"pacific\" + 0.000*\"full\" + '\n",
      "  '0.000*\"earth\" + 0.000*\"recent\"'),\n",
      " (13,\n",
      "  '0.085*\"puzzle\" + 0.066*\"help\" + 0.000*\"meant\" + 0.000*\"drab\" + '\n",
      "  '0.000*\"cottage\" + 0.000*\"study\" + 0.000*\"number\" + 0.000*\"important\" + '\n",
      "  '0.000*\"reader\" + 0.000*\"spruce\"'),\n",
      " (14,\n",
      "  '0.091*\"star\" + 0.071*\"island\" + 0.048*\"soldiers\" + 0.043*\"past\" + '\n",
      "  '0.042*\"articles\" + 0.032*\"continues\" + 0.029*\"our_columnist\" + 0.028*\"stay\" '\n",
      "  '+ 0.028*\"wooed\" + 0.027*\"vice\"'),\n",
      " (15,\n",
      "  '0.121*\"city\" + 0.062*\"away\" + 0.055*\"bodega\" + 0.054*\"since\" + 0.046*\"year\" '\n",
      "  '+ 0.023*\"day\" + 0.000*\"says\" + 0.000*\"monrovia\" + 0.000*\"nonpartisan\" + '\n",
      "  '0.000*\"study\"'),\n",
      " (16,\n",
      "  '0.116*\"avenues\" + 0.101*\"long\" + 0.047*\"million\" + 0.045*\"struggled\" + '\n",
      "  '0.023*\"shiv\" + 0.000*\"things\" + 0.000*\"important\" + 0.000*\"four\" + '\n",
      "  '0.000*\"home\" + 0.000*\"put\"'),\n",
      " (17,\n",
      "  '0.063*\"republicans\" + 0.061*\"hardship\" + 0.052*\"protest\" + 0.045*\"cosplays\" '\n",
      "  '+ 0.031*\"decades\" + 0.029*\"override\" + 0.028*\"movement\" + 0.028*\"unified\" + '\n",
      "  '0.025*\"drama\" + 0.025*\"rage\"'),\n",
      " (18,\n",
      "  '0.000*\"brutality\" + 0.000*\"colonialism\" + 0.000*\"scrutiny\" + '\n",
      "  '0.000*\"highlighted\" + 0.000*\"imminent\" + 0.000*\"pointed\" + 0.000*\"tilt\" + '\n",
      "  '0.000*\"encounter\" + 0.000*\"grant\" + 0.000*\"stan\"'),\n",
      " (19,\n",
      "  '0.085*\"bringing\" + 0.053*\"fuels\" + 0.034*\"attacks\" + 0.000*\"full\" + '\n",
      "  '0.000*\"important\" + 0.000*\"recent\" + 0.000*\"park\" + 0.000*\"pacific\" + '\n",
      "  '0.000*\"view\" + 0.000*\"meant\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the keyword in the 10 topics\n",
    "from pprint import pprint\n",
    "pprint(lda_model.print_topics(num_topics=20, num_words=10))\n",
    "doc_lda = lda_model[corpus]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

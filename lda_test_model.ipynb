{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code has been modified from Craig and Ping's work.\n",
    "References: https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#3importpackages, https://markroxor.github.io/gensim/static/notebooks/lda_training_tips.html, and https://radimrehurek.com/gensim/models/ldamodel.html.\n",
    "\n",
    "New code is from the first link so we can modify it later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import json\n",
    "import os\n",
    "#os.environ['KMP_WARNINGS'] = '0'\n",
    "#os.environ['OMP_NUM_THREADS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "df = pd.read_csv('data/nyt_metadata_cleaned.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that are not needed\n",
    "columns_to_drop = ['web_url', \n",
    "                   'snippet', \n",
    "                   'lead_paragraph', \n",
    "                   'print_section', \n",
    "                   'print_page', \n",
    "                   'source', \n",
    "                   'multimedia', \n",
    "                   'news_desk',\n",
    "                   'byline',\n",
    "                   '_id',\n",
    "                   'uri',\n",
    "                   'subsection_name',\n",
    "                   'word_count',\n",
    "                   'keywords']\n",
    "df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Drop rows with missing abstracts\n",
    "drop_rows = df[df['abstract'].isnull()].index\n",
    "df.drop(drop_rows, inplace=True)\n",
    "\n",
    "# Change the date column to datetime\n",
    "df['pub_date'] = pd.to_datetime(df['pub_date'])\n",
    "\n",
    "# Change the abstract column to string\n",
    "df['abstract'] = df['abstract'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the 'main' value from JSON-like strings\n",
    "def extract_main(headline_str):\n",
    "    try:\n",
    "        # Safely evaluate the string to convert it to a dictionary\n",
    "        json_dict = ast.literal_eval(headline_str)\n",
    "        # Access and return the 'main' key\n",
    "        return json_dict.get('main', None)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return None\n",
    "\n",
    "# Apply the function to the 'headline' column\n",
    "df['headline'] = df['headline'].apply(extract_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>abstract</th>\n",
       "      <th>headline</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>document_type</th>\n",
       "      <th>section_name</th>\n",
       "      <th>type_of_material</th>\n",
       "      <th>tokenized_abstract</th>\n",
       "      <th>tokenized_lead_par</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1813</td>\n",
       "      <td>economic hardship climate change political ins...</td>\n",
       "      <td>Title 42 Is Gone, but Not the Conditions Drivi...</td>\n",
       "      <td>2023-05-15 01:24:42+00:00</td>\n",
       "      <td>article</td>\n",
       "      <td>U.S.</td>\n",
       "      <td>News</td>\n",
       "      <td>['economic', 'hardship', 'climate', 'change', ...</td>\n",
       "      <td>['relative', 'quiet', 'has', 'prevailed', 'alo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1814</td>\n",
       "      <td>election night america stay away from the bode...</td>\n",
       "      <td>‘Succession’ Season 4, Episode 8 Recap: The Wi...</td>\n",
       "      <td>2023-05-15 02:01:05+00:00</td>\n",
       "      <td>article</td>\n",
       "      <td>Arts</td>\n",
       "      <td>News</td>\n",
       "      <td>['election', 'night', 'america', 'stay', 'away...</td>\n",
       "      <td>['the', 'day', 'before', 'logan', 'roy', 'died...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1815</td>\n",
       "      <td>tom stressed dress shoes shiv hides beneath la...</td>\n",
       "      <td>‘Succession’ Style, Episode 8: Some People Jus...</td>\n",
       "      <td>2023-05-15 02:15:04+00:00</td>\n",
       "      <td>article</td>\n",
       "      <td>Style</td>\n",
       "      <td>News</td>\n",
       "      <td>['tom', 'stressed', 'dress', 'shoes', 'shiv', ...</td>\n",
       "      <td>['this', 'article', 'contains', 'spoilers', 'f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1816</td>\n",
       "      <td>corrections appeared print monday may 2023</td>\n",
       "      <td>No Corrections: May 15, 2023</td>\n",
       "      <td>2023-05-15 03:55:48+00:00</td>\n",
       "      <td>article</td>\n",
       "      <td>Corrections</td>\n",
       "      <td>News</td>\n",
       "      <td>['corrections', 'appeared', 'print', 'monday',...</td>\n",
       "      <td>['errors', 'are', 'corrected', 'during', 'the'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1818</td>\n",
       "      <td>the year old french basketball star the most h...</td>\n",
       "      <td>Everybody Wants Victor Wembanyama. He Wants to...</td>\n",
       "      <td>2023-05-15 04:01:13+00:00</td>\n",
       "      <td>article</td>\n",
       "      <td>Sports</td>\n",
       "      <td>News</td>\n",
       "      <td>['the', 'year', 'old', 'french', 'basketball',...</td>\n",
       "      <td>['boris', 'diaw', 'was', 'passing', 'through',...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0  \\\n",
       "0             0        1813   \n",
       "1             1        1814   \n",
       "2             2        1815   \n",
       "3             3        1816   \n",
       "4             5        1818   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  economic hardship climate change political ins...   \n",
       "1  election night america stay away from the bode...   \n",
       "2  tom stressed dress shoes shiv hides beneath la...   \n",
       "3         corrections appeared print monday may 2023   \n",
       "4  the year old french basketball star the most h...   \n",
       "\n",
       "                                            headline  \\\n",
       "0  Title 42 Is Gone, but Not the Conditions Drivi...   \n",
       "1  ‘Succession’ Season 4, Episode 8 Recap: The Wi...   \n",
       "2  ‘Succession’ Style, Episode 8: Some People Jus...   \n",
       "3                       No Corrections: May 15, 2023   \n",
       "4  Everybody Wants Victor Wembanyama. He Wants to...   \n",
       "\n",
       "                   pub_date document_type section_name type_of_material  \\\n",
       "0 2023-05-15 01:24:42+00:00       article         U.S.             News   \n",
       "1 2023-05-15 02:01:05+00:00       article         Arts             News   \n",
       "2 2023-05-15 02:15:04+00:00       article        Style             News   \n",
       "3 2023-05-15 03:55:48+00:00       article  Corrections             News   \n",
       "4 2023-05-15 04:01:13+00:00       article       Sports             News   \n",
       "\n",
       "                                  tokenized_abstract  \\\n",
       "0  ['economic', 'hardship', 'climate', 'change', ...   \n",
       "1  ['election', 'night', 'america', 'stay', 'away...   \n",
       "2  ['tom', 'stressed', 'dress', 'shoes', 'shiv', ...   \n",
       "3  ['corrections', 'appeared', 'print', 'monday',...   \n",
       "4  ['the', 'year', 'old', 'french', 'basketball',...   \n",
       "\n",
       "                                  tokenized_lead_par  \n",
       "0  ['relative', 'quiet', 'has', 'prevailed', 'alo...  \n",
       "1  ['the', 'day', 'before', 'logan', 'roy', 'died...  \n",
       "2  ['this', 'article', 'contains', 'spoilers', 'f...  \n",
       "3  ['errors', 'are', 'corrected', 'during', 'the'...  \n",
       "4  ['boris', 'diaw', 'was', 'passing', 'through',...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 2000 of the abstracts for the analysis\n",
    "abstracts = df['abstract'][:2000]\n",
    "docs = abstracts.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the documents.\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Split the documents into tokens.\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for idx in range(len(docs)):\n",
    "    docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "# Remove numbers, but not words that contain numbers.\n",
    "docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "\n",
    "# Remove words that are only one character.\n",
    "docs = [[token for token in doc if len(token) > 1] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['economic',\n",
       "  'hardship',\n",
       "  'climate',\n",
       "  'change',\n",
       "  'political',\n",
       "  'instability',\n",
       "  'and',\n",
       "  'gang',\n",
       "  'violence',\n",
       "  'will',\n",
       "  'continue',\n",
       "  'spur',\n",
       "  'emigration',\n",
       "  'from',\n",
       "  'many',\n",
       "  'corners',\n",
       "  'the',\n",
       "  'world'],\n",
       " ['election',\n",
       "  'night',\n",
       "  'america',\n",
       "  'stay',\n",
       "  'away',\n",
       "  'from',\n",
       "  'the',\n",
       "  'bodega',\n",
       "  'sushi']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/ale/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize the documents.\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize all words in documents.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute bigrams.\n",
    "\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['economic',\n",
       "  'hardship',\n",
       "  'climate',\n",
       "  'change',\n",
       "  'political',\n",
       "  'instability',\n",
       "  'and',\n",
       "  'gang',\n",
       "  'violence',\n",
       "  'will',\n",
       "  'continue',\n",
       "  'spur',\n",
       "  'emigration',\n",
       "  'from',\n",
       "  'many',\n",
       "  'corner',\n",
       "  'the',\n",
       "  'world']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rare and common tokens.\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['election',\n",
       " 'night',\n",
       " 'america',\n",
       " 'stay',\n",
       " 'away',\n",
       " 'from',\n",
       " 'the',\n",
       " 'bodega',\n",
       " 'sushi']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/ale/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define the stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stopwords from each document\n",
    "filtered_docs = [[word for word in doc if word not in stop_words] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['economic',\n",
       "  'hardship',\n",
       "  'climate',\n",
       "  'change',\n",
       "  'political',\n",
       "  'instability',\n",
       "  'gang',\n",
       "  'violence',\n",
       "  'continue',\n",
       "  'spur',\n",
       "  'emigration',\n",
       "  'many',\n",
       "  'corner',\n",
       "  'world']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_docs[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concatenate each entry into a single string after removing stopwords\n",
    "#cleaned_docs = [' '.join(doc) for doc in filtered_docs]\n",
    "\n",
    "## Example of how to print the filtered and concatenated result for the first document\n",
    "#print(cleaned_docs[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize data.\n",
    "\n",
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)], [(2, 1), (7, 1)]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(filtered_docs)\n",
    "\n",
    "# Create Corpus\n",
    "texts = filtered_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('change', 1),\n",
       "  ('climate', 1),\n",
       "  ('continue', 1),\n",
       "  ('corner', 1),\n",
       "  ('economic', 1),\n",
       "  ('emigration', 1),\n",
       "  ('gang', 1)]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.099*\"led\" + 0.059*\"month\" + 0.051*\"vice\" + 0.050*\"ability\" + '\n",
      "  '0.042*\"trinidad\" + 0.040*\"struggled\" + 0.040*\"left\" + 0.038*\"made\" + '\n",
      "  '0.032*\"nation\" + 0.000*\"drab\"'),\n",
      " (1,\n",
      "  '0.084*\"article\" + 0.081*\"spur\" + 0.049*\"number\" + 0.042*\"gas\" + '\n",
      "  '0.042*\"reality\" + 0.041*\"put\" + 0.038*\"ha_been\" + 0.036*\"liquefied\" + '\n",
      "  '0.031*\"first\" + 0.028*\"many\"'),\n",
      " (2,\n",
      "  '0.242*\"reflect\" + 0.066*\"drama\" + 0.051*\"massacre\" + 0.044*\"shoe\" + '\n",
      "  '0.036*\"nine\" + 0.034*\"fx\" + 0.027*\"medium\" + 0.026*\"hillsong\" + '\n",
      "  '0.000*\"visceral\" + 0.000*\"future\"'),\n",
      " (3,\n",
      "  '0.096*\"instability\" + 0.062*\"african\" + 0.058*\"year\" + 0.047*\"new\" + '\n",
      "  '0.045*\"layer\" + 0.043*\"new_york\" + 0.042*\"old\" + 0.038*\"principal\" + '\n",
      "  '0.034*\"prospect\" + 0.031*\"pressure\"'),\n",
      " (4,\n",
      "  '0.100*\"believed\" + 0.052*\"battle\" + 0.000*\"echo\" + 0.000*\"house\" + '\n",
      "  '0.000*\"view\" + 0.000*\"michael\" + 0.000*\"inspire\" + 0.000*\"assault\" + '\n",
      "  '0.000*\"four\" + 0.000*\"drab\"'),\n",
      " (5,\n",
      "  '0.098*\"lebron\" + 0.093*\"basketball\" + 0.056*\"war\" + 0.046*\"fazed\" + '\n",
      "  '0.044*\"night\" + 0.043*\"legislature\" + 0.042*\"hyped\" + 0.028*\"james\" + '\n",
      "  '0.018*\"many\" + 0.014*\"long\"'),\n",
      " (6,\n",
      "  '0.055*\"member\" + 0.000*\"assault\" + 0.000*\"four\" + 0.000*\"spruce\" + '\n",
      "  '0.000*\"earth\" + 0.000*\"northwest\" + 0.000*\"close\" + 0.000*\"drab\" + '\n",
      "  '0.000*\"man\" + 0.000*\"reader\"'),\n",
      " (7,\n",
      "  '0.156*\"say\" + 0.064*\"sentence\" + 0.060*\"gang\" + 0.055*\"america\" + '\n",
      "  '0.039*\"reading\" + 0.038*\"ten\" + 0.036*\"spy\" + 0.034*\"shiv\" + '\n",
      "  '0.031*\"soldier\" + 0.030*\"bringing\"'),\n",
      " (8,\n",
      "  '0.131*\"million\" + 0.120*\"punishing\" + 0.078*\"documentary\" + 0.053*\"raise\" + '\n",
      "  '0.031*\"extreme\" + 0.029*\"worker\" + 0.000*\"reader\" + 0.000*\"destructive\" + '\n",
      "  '0.000*\"debut\" + 0.000*\"four\"'),\n",
      " (9,\n",
      "  '0.255*\"cosplays\" + 0.079*\"lady\" + 0.079*\"dress\" + 0.073*\"happening\" + '\n",
      "  '0.067*\"beneath\" + 0.040*\"stressed\" + 0.027*\"lie\" + 0.026*\"china\" + '\n",
      "  '0.021*\"central\" + 0.000*\"man\"'),\n",
      " (10,\n",
      "  '0.127*\"leader\" + 0.083*\"find\" + 0.047*\"effort\" + 0.043*\"gain\" + 0.038*\"com\" '\n",
      "  '+ 0.038*\"showtime\" + 0.035*\"away\" + 0.028*\"ending\" + 0.000*\"reader\" + '\n",
      "  '0.000*\"cottage\"'),\n",
      " (11,\n",
      "  '0.172*\"emigration\" + 0.000*\"spruce\" + 0.000*\"four\" + 0.000*\"view\" + '\n",
      "  '0.000*\"visceral\" + 0.000*\"michael\" + 0.000*\"bedroom\" + 0.000*\"across\" + '\n",
      "  '0.000*\"echo\" + 0.000*\"mountain\"'),\n",
      " (12,\n",
      "  '0.118*\"natural\" + 0.072*\"climate\" + 0.060*\"died\" + 0.055*\"even\" + '\n",
      "  '0.047*\"quickly\" + 0.047*\"day\" + 0.031*\"continues\" + 0.026*\"publishing\" + '\n",
      "  '0.000*\"echo\" + 0.000*\"four\"'),\n",
      " (13,\n",
      "  '0.090*\"corner\" + 0.066*\"economic\" + 0.000*\"closet\" + 0.000*\"drab\" + '\n",
      "  '0.000*\"pacific\" + 0.000*\"mountain\" + 0.000*\"incite\" + 0.000*\"across\" + '\n",
      "  '0.000*\"man\" + 0.000*\"view\"'),\n",
      " (14,\n",
      "  '0.272*\"avenue\" + 0.065*\"decade\" + 0.049*\"union\" + 0.039*\"adjust\" + '\n",
      "  '0.038*\"pastor\" + 0.037*\"scandal\" + 0.030*\"come\" + 0.015*\"passed\" + '\n",
      "  '0.000*\"four\" + 0.000*\"park\"'),\n",
      " (15,\n",
      "  '0.093*\"attack\" + 0.093*\"wooed\" + 0.085*\"overseas\" + 0.061*\"several\" + '\n",
      "  '0.000*\"four\" + 0.000*\"destructive\" + 0.000*\"view\" + 0.000*\"northwest\" + '\n",
      "  '0.000*\"full\" + 0.000*\"mountain\"'),\n",
      " (16,\n",
      "  '0.163*\"question\" + 0.062*\"time\" + 0.044*\"democratic\" + 0.043*\"fossil\" + '\n",
      "  '0.033*\"study\" + 0.030*\"override\" + 0.028*\"word\" + 0.024*\"protect\" + '\n",
      "  '0.023*\"may\" + 0.000*\"four\"'),\n",
      " (17,\n",
      "  '0.143*\"change\" + 0.062*\"turtleneck\" + 0.056*\"hide\" + 0.043*\"hopeful\" + '\n",
      "  '0.037*\"sushi\" + 0.036*\"appeared\" + 0.034*\"world\" + 0.031*\"continue\" + '\n",
      "  '0.027*\"bakhmut\" + 0.025*\"nba\"'),\n",
      " (18,\n",
      "  '0.154*\"unified\" + 0.083*\"exporter\" + 0.056*\"vetoed\" + 0.052*\"since\" + '\n",
      "  '0.052*\"recent\" + 0.036*\"committed\" + 0.036*\"fuel\" + 0.032*\"falling\" + '\n",
      "  '0.017*\"republican\" + 0.014*\"remains\"'),\n",
      " (19,\n",
      "  '0.147*\"weather\" + 0.082*\"island\" + 0.076*\"output\" + 0.056*\"gop\" + '\n",
      "  '0.050*\"ban\" + 0.029*\"citizen\" + 0.019*\"election\" + 0.000*\"mountain\" + '\n",
      "  '0.000*\"incite\" + 0.000*\"magee\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the keyword in the 10 topics\n",
    "from pprint import pprint\n",
    "pprint(lda_model.print_topics(num_topics=20, num_words=10))\n",
    "doc_lda = lda_model[corpus]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

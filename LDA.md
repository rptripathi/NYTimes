# A Summary of Latent Dirichlet Allocation

### Vectorization

In this model, a document (or article) is viewed as a sequence of words drawn from a suitable set, called the *vocabulary*.  
Intuitively, documents about a particular topic are more likely to use certain words of the voabulary than others. With that in mind, one can think of a topic as a probability distribution on the vocabulary, or equivalently a collection of parameters $\varphi_i$ expressing the probability of the $i$-th word of being used in an article about that topic. With that in mind, in this model a *topic* will be represented as a vector $\boldsymbol{\varphi}$ of dimension $V$, the size of the vocabulary, such that:
- The entries of $\boldsymbol{\varphi}$ are real numbers between $0$ and $1$;  
- The sum of all entries of $\boldsymbol{\varphi}$ is equal to $1$.  

A document of length $N$ about a single topic $\boldsymbol(\varphi)$ is then imagined as having been generated by $N$ sequential and independent draws from the vocabulary distributed as the probability represented by $\boldsymbol(\varphi)$.  
More generally, however, one could imagine that each document might cover a mixture of different topics
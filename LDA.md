# A Summary of Latent Dirichlet Allocation

## The model

### Generating a document

In this model, a document (or article) is viewed as a sequence of words drawn from a suitable set, called the *vocabulary*.  
Intuitively, documents about a particular topic are more likely to use certain words of the voabulary than others. With that in mind, one can think of a topic as a probability distribution on the vocabulary, i.e., a collection of parameters $\varphi_{w}$ expressing the probability of the $w$-th word being used in an article about that topic. With that in mind, in this model a *topic* will be represented as a vector $\boldsymbol{\varphi}$ of dimension $V$, the size of the vocabulary, such that:
- The entries of $\boldsymbol{\varphi}$ are real numbers between $0$ and $1$;  
- The sum of all entries of $\boldsymbol{\varphi}$ is equal to $1$.  

A document of length $N$ about a single topic $\boldsymbol{\varphi}$ is then imagined as having been generated by $N$ sequential and independent draws from the vocabulary, all distributed as the probability represented by $\boldsymbol{\varphi}$. In general, however, a document may contain several topics, each with a different weight&mdash;for exmaple, an article may be 75% about politics, 20% about economics, and 5% about sociology. To make this idea precise, assume that a set of $K$ topics $\boldsymbol{\varphi}_{k}$ is fixed. To a document will then correspond a weight $\theta_{k}$ for each topic $\boldsymbol{\varphi}_{k}$, thus forming a $K$-dimensional vector $\boldsymbol{\theta}$. Just like for topics, we can think of such a $\boldsymbol{\theta}$ as a probability distribution and assume its entries are real numbers between $0$ and $1$ with sum equal to $1$.  
In this model, each word of a document of length $N$ and given topic distribution $\boldsymbol{\theta}$ is then generated as follows:
- First, draw a topic, say $\boldsymbol{\varphi}_{k}$ from the distribution $\boldsymbol{\theta}$;  
- Second, draw a word form $V$ using the distribution $\boldsymbol{\varphi}_{k}$.  

### Generating a corpus

Now we have a model of how a document is generated from a collection of known topics and a given topic distribution, but we also need to describe how a whole *corpus* (collection) of documents is generated. LDA works by assuming that a corpus is generated by the following steps:
1. Select a collection of topics $\boldsymbol{\varphi}_{k}$;
1. Choosing a topic distribution $\boldsymbol{\theta}_{i}$ for each document;
1. Generate each document itself by the process sketched above.

Crucially, since we have no information on how the topics and topic distribution are generated, we must assume that these are drawn *randomly* and specify how we expect them to be distributed *a priori*. The typical prior used in this situation is called a *Dirichlet distribution*, often denoted by $\operatorname{Dir}$. This object depends on a hyperparameter which, under the appropriate conditions, captures the idea that only a few topics should be present in each document, and only a small portion of all words should be present in each topic.  

With this in mind, in order to generate a corpus of documents we need to assume given
- A vocabulary of size $N$;
- A number $K$ of topics;
- A number $M$ of documents and, for each $i$ between $1$ and $M$, a number $N_{i}$ of words for the $i$-th document;
- Two Dirichlet distributions of hyperparameters $\alpha$ and $\beta$.

Having fixed this data, the generative process proceeds as follows:
1. Draw $K$ topics $\boldsymbol{\varphi}_{k}$ from the distribution $\operatorname{Dir}(\beta)$. These will remain fixed throughout the whole generative process.
1. For each document, draw a topic distribution $\boldsymbol{\theta}_{i}$ from the distribution $\operatorname{Dir}(\alpha)$, where $i$ represents the index of a document within the corpus.
1. For each $i$ between $1$ and $M$ (representing a document) and $j$ between $1$ and $N_{i}$ (representing the position of a word within the document), draw a topic from the distribution $\boldsymbol{\theta}_{i}$ corresponding to that document, and call $z_{ij}$ its index.
1. For each $i$ and $j$ as above, draw a word $w$ from the distribution $\boldsymbol{\varphi}_{z_{ij}}$, and write it in the $j$-th position of the $i$-th document.  

## Data preparation

The following pre-processing steps are taken from [this article](https://markroxor.github.io/gensim/static/notebooks/lda_training_tips.html).
- Split each document as a list of words, ignoring cases and disregarding all numbers.
- Lemmatize all words, i.e., reduce all forms of each word (building, build, built...) to one preferred representative (build). This can be achieved using the `WordNetLemmatizer` python class from the `nltk.stem.wordnet` library.
- Indentify any bigrams, i.e., pairs of words that are often used as parts of a single expression, such as *machine learning*, *computer science*, and *damage control*. This can be useful for disambiguation of words used differently in specific contexts. Again, there is a class `Phrases` of the `gensim.models` libraries that implements this.
- Disregard any words that appear too frequently or too rarely. The former, including prepositions, pronouns, and words of general use like *have* and *be*, are of little use for identifying semantic areas as they are likely used uniformly across all topics. On the other hand, words that are too specific to a particular article (e.g., the name of an interviewee or the address of a one-time event) will probably fail to draw a connection between articles on similar topics.

Once the corpus is pre-processed, the vocabulary can be defined as the list of unique words.

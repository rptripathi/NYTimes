# Goals and Methods

The following is a preliminary description of the goals of our project. Details may change as our work progresses.

## Goals

Analyze New York Times articles by topic and explore applications such as
- Building a recommendation system based on topic similarity;
- Analyzing correlations between topics;
- Verifying accuracy of keywords and/or suggesting additional ones;
- Evaluating trends in topic frequency.

### The dataset

We will be using the (official API provided by the New York Times)[https://developer.nytimes.com/apis] to scrape the metadata available for all articles published between May 15th, 2023 and May 14th, 2024. The data associated with each article includes
- Title;
- Headline;
- Abstract;
- URL;
- Section and page in which the article was published;
- Keywords.
We plan to restrict our attention specifically to articles from a relative short period of time, e.g., the last year or two.

### The stakeholders

As stakeholders for this project, we envision our team members (Ravi Tripathi, Touseef Haider, Ping Wan, Schinella D'Souza, Alessandro MalusÃ , Craig Franze) and the New York Times, or potentially any online news platform.

### The Key Performance Indicators (KPI)

- Consistency of topic detection from different portions of the same article;
- Consistency between the topics detected in a given article and the associated keywords.

## Methods

### Quick introduction to LDA

We will approach our problem using a method called Latent Dirichlet Allocation (LDA). The latter is an example of *generative statistical model*, meaning that it starts by first describing a stochastic process by which the data at hand (in our case, a collection of articles) would have been generated given a set of unknown parameters. Once the mode is set up, the goal is to use statistical inference to reverse-engineer values of the parameters which are most likely to have produced the real data.  

In short, LDA assumes that each document in the corpus has been generated by first choosing a certain set of topics, each with a certain weight, and then sequentially drawing each word from a probability distribution determined by the chosen mix of topics. In this model, the underlying unknown (*latent*) parameters are the mix of topics in each document and the probability of each word appearing in a given topic.  
In order to apply statistical inference to estimate these parameters, one needs to make some assumption on the *a priori* shape of the distribution of topics in each document and words in each topic. The natural choice is a *sparse Dirichlet* distribution (hence the "D" in "LDA"), which captures the intuitive idea that only a few topics should appear in each article, and only a small portion of all words should appear in each topic.  

### Application to our dataset

The vast majority (>90%) of data points in our set come with an abstract, a snippet, and a lead paragraph. Following are a few possible ways to apply LDA to our dataset.
- Concatenate the abstract, snippet, and lead paragraph of each article into a single document, then apply LDA to the corpus of documents thus obtained.
- Collect all abstracts, snippets, and lead paragraphs into a single corpus, then apply LDA. This way, the three component of each article are assigned a mix of topics independently of the other two, leading to potentially different topic distributions. We might then study the correlation between the three distributions to evaluate the quality of our model.
- Organize abstracts, snippets, and lead paragraphs into *three* separate corpora, then apply LDA to each. This will result in three separate sets of topics, and each article will be assigned topics from each set independently. This would require further study of the correlations between topics in each set and between the three distinct topic assignments of each article.

After applying each of the three approaches sketched above, one should study any correlations between the topics found and the keywords assigned to each article in the dataset. This could also be used to label the topics found by LDA, since the algorithm only produces probability distributions on words without really understanding their meaning or relations.

### Validation

A common metric used to assess the performance of probability models (of which LDA is an example) is the notion of **perplexity**. Simply put, perplexity measures how "surprised" a probability model is to see a particular outcome after performing a test: the more improbable the result, the higher the perplexity. More formally, the perplexity of a probability model on an outcome is defined as the reciprocal of its estimated probability (or the logarithm of that number). In the case of multiple tests, it is customary to consider the geometric mean of the individual perplexities (or the arithmetic mean in the logarithmic version).  
Perplexity is a good metric for (cross) validation of unsupervised training of a probabilistic model such as LDA.

## Recommendation engine

The LDA algorithm will assign to each article a distribution of topics. This can be used to measure "topic distance" between articles: Intuitively, two articles will be close to each other if their topic distribution is similar. Following are a few ideas on how to implement a recommendation engine:
- **Nearest article.** Given an article, simply recommend the nearest article.
- **Include publication date.** Since the reader might prefer an article that was published either recently, or at a similar time as the one they just read. Keeping that in mind, the distance between articles could be adjusted to account for publication date.
- **Random variability.** Recommending the nearest article deterministically may give rise to undesired effects, e.g., getting stuck in a cluster of articles with extremely similar topics or in a loop of few articles. Instead, recommendations could be drawn randomly from a suitable distribution based on distance.
- **Recommendation with memory.** Our recommendation algorithm could be based on history of previously read articles instead of the current one alone. This could could be used to avoid recommending an article that had already been read, and possibly recommend articles that are close to previously read ones even if they are far from the current one.
- **Rating system.** Knowing that a reader accessed an article does not necessarily mean they enjoyed it. The recommendation system could be improved by keeping track of the user's preferences, for example by developing a rating system or by tracking what proportion of each article was displayed on the reader's screen and for how long.

### Potential issues:
- Topic similarity alone may not be a sufficient indicator for accurate recommendations. For example, this system might favor articles in large clusters over more isolated ones, regardless of their quality. In fact, if such a cluster contains many obsolete articles and few accurate ones, this system would not only give visibility to the less relevant ones, but also fail to identify the better articles within the cluster. In addition, readers will be interested in several topics and want recommendations to be based on that, and not only the last read article.
- The performance of the recommendation engine might be very difficult to evaluate, due to the lack of feedback from the readers.

## Further directions

The performance of the recommendation system could be improved by including data on user consumption.  
On the one hand, general statistics on individual documents (how many times it was visualized, by how many users, for how long, what proportion was actually displayed...) might help the recommender favor trending articles and disregard obsolete ones.  
On the other hand, knowing a particular user's preferences would help varying the recommendations and tailor them to one's specific interests while at the same time avoiding bringing up the same article multiple times. Moreover, access to data on the user's interaction with an article would be helpful to evaluate if a recommendation was effective, and unltimately to estimate performance of the engine.  

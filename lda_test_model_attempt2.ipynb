{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code has been modified from Craig and Ping's work.\n",
    "References: https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#3importpackages, https://markroxor.github.io/gensim/static/notebooks/lda_training_tips.html, and https://radimrehurek.com/gensim/models/ldamodel.html.\n",
    "\n",
    "New code is from the first link so we can modify it later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import json\n",
    "import os\n",
    "#os.environ['KMP_WARNINGS'] = '0'\n",
    "#os.environ['OMP_NUM_THREADS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "df = pd.read_csv('data/nyt_metadata.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that are not needed\n",
    "columns_to_drop = ['web_url', \n",
    "                   'snippet', \n",
    "                   'lead_paragraph', \n",
    "                   'print_section', \n",
    "                   'print_page', \n",
    "                   'source', \n",
    "                   'multimedia', \n",
    "                   'news_desk',\n",
    "                   'byline',\n",
    "                   '_id',\n",
    "                   'uri',\n",
    "                   'subsection_name',\n",
    "                   'word_count',\n",
    "                   'keywords']\n",
    "df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Drop rows with missing abstracts\n",
    "drop_rows = df[df['abstract'].isnull()].index\n",
    "df.drop(drop_rows, inplace=True)\n",
    "\n",
    "# Change the date column to datetime\n",
    "df['pub_date'] = pd.to_datetime(df['pub_date'])\n",
    "\n",
    "# Change the abstract column to string\n",
    "df['abstract'] = df['abstract'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the 'main' value from JSON-like strings\n",
    "def extract_main(headline_str):\n",
    "    try:\n",
    "        # Safely evaluate the string to convert it to a dictionary\n",
    "        json_dict = ast.literal_eval(headline_str)\n",
    "        # Access and return the 'main' key\n",
    "        return json_dict.get('main', None)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return None\n",
    "\n",
    "# Apply the function to the 'headline' column\n",
    "df['headline'] = df['headline'].apply(extract_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>abstract</th>\n",
       "      <th>headline</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>document_type</th>\n",
       "      <th>section_name</th>\n",
       "      <th>type_of_material</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1813</td>\n",
       "      <td>Economic hardship, climate change, political i...</td>\n",
       "      <td>Title 42 Is Gone, but Not the Conditions Drivi...</td>\n",
       "      <td>2023-05-15 01:24:42+00:00</td>\n",
       "      <td>article</td>\n",
       "      <td>U.S.</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1814</td>\n",
       "      <td>It’s election night in America. Stay away from...</td>\n",
       "      <td>‘Succession’ Season 4, Episode 8 Recap: The Wi...</td>\n",
       "      <td>2023-05-15 02:01:05+00:00</td>\n",
       "      <td>article</td>\n",
       "      <td>Arts</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1815</td>\n",
       "      <td>Tom is stressed in dress shoes, Shiv hides ben...</td>\n",
       "      <td>‘Succession’ Style, Episode 8: Some People Jus...</td>\n",
       "      <td>2023-05-15 02:15:04+00:00</td>\n",
       "      <td>article</td>\n",
       "      <td>Style</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1816</td>\n",
       "      <td>No corrections appeared in print on Monday, Ma...</td>\n",
       "      <td>No Corrections: May 15, 2023</td>\n",
       "      <td>2023-05-15 03:55:48+00:00</td>\n",
       "      <td>article</td>\n",
       "      <td>Corrections</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1817</td>\n",
       "      <td>Quotation of the Day for Monday, May 15, 2023.</td>\n",
       "      <td>Quotation of the Day: When Your Champions Leag...</td>\n",
       "      <td>2023-05-15 03:55:57+00:00</td>\n",
       "      <td>article</td>\n",
       "      <td>Corrections</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           abstract  \\\n",
       "0        1813  Economic hardship, climate change, political i...   \n",
       "1        1814  It’s election night in America. Stay away from...   \n",
       "2        1815  Tom is stressed in dress shoes, Shiv hides ben...   \n",
       "3        1816  No corrections appeared in print on Monday, Ma...   \n",
       "4        1817     Quotation of the Day for Monday, May 15, 2023.   \n",
       "\n",
       "                                            headline  \\\n",
       "0  Title 42 Is Gone, but Not the Conditions Drivi...   \n",
       "1  ‘Succession’ Season 4, Episode 8 Recap: The Wi...   \n",
       "2  ‘Succession’ Style, Episode 8: Some People Jus...   \n",
       "3                       No Corrections: May 15, 2023   \n",
       "4  Quotation of the Day: When Your Champions Leag...   \n",
       "\n",
       "                   pub_date document_type section_name type_of_material  \n",
       "0 2023-05-15 01:24:42+00:00       article         U.S.             News  \n",
       "1 2023-05-15 02:01:05+00:00       article         Arts             News  \n",
       "2 2023-05-15 02:15:04+00:00       article        Style             News  \n",
       "3 2023-05-15 03:55:48+00:00       article  Corrections             News  \n",
       "4 2023-05-15 03:55:57+00:00       article  Corrections             News  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use all of the abstracts for the analysis\n",
    "abstracts = df['abstract']\n",
    "docs = abstracts.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42623"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the documents.\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Split the documents into tokens.\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for idx in range(len(docs)):\n",
    "    docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "# Remove numbers, but not words that contain numbers.\n",
    "docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "\n",
    "# Remove words that are only one character.\n",
    "docs = [[token for token in doc if len(token) > 1] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['economic',\n",
       "  'hardship',\n",
       "  'climate',\n",
       "  'change',\n",
       "  'political',\n",
       "  'instability',\n",
       "  'and',\n",
       "  'gang',\n",
       "  'violence',\n",
       "  'will',\n",
       "  'continue',\n",
       "  'to',\n",
       "  'spur',\n",
       "  'emigration',\n",
       "  'from',\n",
       "  'many',\n",
       "  'corners',\n",
       "  'of',\n",
       "  'the',\n",
       "  'world'],\n",
       " ['it',\n",
       "  'election',\n",
       "  'night',\n",
       "  'in',\n",
       "  'america',\n",
       "  'stay',\n",
       "  'away',\n",
       "  'from',\n",
       "  'the',\n",
       "  'bodega',\n",
       "  'sushi']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/schinella/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define the stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stopwords from each document\n",
    "filtered_docs = [[word for word in doc if word not in stop_words] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['economic',\n",
       "  'hardship',\n",
       "  'climate',\n",
       "  'change',\n",
       "  'political',\n",
       "  'instability',\n",
       "  'gang',\n",
       "  'violence',\n",
       "  'continue',\n",
       "  'spur',\n",
       "  'emigration',\n",
       "  'many',\n",
       "  'corners',\n",
       "  'world']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_docs[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute bigrams.\n",
    "\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "# bigram[filtered_docs[0]]\n",
    "\n",
    "for idx in range(len(docs)):\n",
    "    filtered_docs[idx] = bigram[filtered_docs[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "# bigram = Phrases(docs, min_count=20)\n",
    "# for idx in range(len(docs)):\n",
    "#    for token in bigram[docs[idx]]:\n",
    "#        if '_' in token:\n",
    "#            # Token is a bigram, add to document.\n",
    "#            filtered_docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['economic',\n",
       "  'hardship',\n",
       "  'climate_change',\n",
       "  'political',\n",
       "  'instability',\n",
       "  'gang',\n",
       "  'violence',\n",
       "  'continue',\n",
       "  'spur',\n",
       "  'emigration',\n",
       "  'many',\n",
       "  'corners',\n",
       "  'world'],\n",
       " ['election', 'night', 'america', 'stay', 'away', 'bodega', 'sushi'],\n",
       " ['tom',\n",
       "  'stressed',\n",
       "  'dress',\n",
       "  'shoes',\n",
       "  'shiv',\n",
       "  'hides',\n",
       "  'beneath',\n",
       "  'layers',\n",
       "  'lies',\n",
       "  'turtleneck',\n",
       "  'willa',\n",
       "  'cosplays',\n",
       "  'first_lady',\n",
       "  'hopeful'],\n",
       " ['corrections_appeared', 'print', 'monday', 'may'],\n",
       " ['quotation', 'day', 'monday', 'may']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_docs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42623"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/schinella/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize the documents.\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/schinella/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize all words in documents.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "fitlered_docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in filtered_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42623"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['economic',\n",
       "  'hardship',\n",
       "  'climate_change',\n",
       "  'political',\n",
       "  'instability',\n",
       "  'gang',\n",
       "  'violence',\n",
       "  'continue',\n",
       "  'spur',\n",
       "  'emigration',\n",
       "  'many',\n",
       "  'corners',\n",
       "  'world'],\n",
       " ['election', 'night', 'america', 'stay', 'away', 'bodega', 'sushi'],\n",
       " ['tom',\n",
       "  'stressed',\n",
       "  'dress',\n",
       "  'shoes',\n",
       "  'shiv',\n",
       "  'hides',\n",
       "  'beneath',\n",
       "  'layers',\n",
       "  'lies',\n",
       "  'turtleneck',\n",
       "  'willa',\n",
       "  'cosplays',\n",
       "  'first_lady',\n",
       "  'hopeful'],\n",
       " ['corrections_appeared', 'print', 'monday', 'may'],\n",
       " ['quotation', 'day', 'monday', 'may']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_docs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rare and common tokens.\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(filtered_docs)\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['economic',\n",
       "  'hardship',\n",
       "  'climate_change',\n",
       "  'political',\n",
       "  'instability',\n",
       "  'gang',\n",
       "  'violence',\n",
       "  'continue',\n",
       "  'spur',\n",
       "  'emigration',\n",
       "  'many',\n",
       "  'corners',\n",
       "  'world']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_docs[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concatenate each entry into a single string after removing stopwords\n",
    "#cleaned_docs = [' '.join(doc) for doc in filtered_docs]\n",
    "\n",
    "## Example of how to print the filtered and concatenated result for the first document\n",
    "#print(cleaned_docs[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize data.\n",
    "\n",
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in filtered_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
       " [(8, 1), (9, 1), (10, 1), (11, 1), (12, 1)]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(filtered_docs)\n",
    "\n",
    "# Create Corpus\n",
    "texts = filtered_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('climate_change', 1),\n",
       "  ('continue', 1),\n",
       "  ('corners', 1),\n",
       "  ('economic', 1),\n",
       "  ('emigration', 1),\n",
       "  ('gang', 1),\n",
       "  ('hardship', 1),\n",
       "  ('instability', 1)]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.228*\"across\" + 0.062*\"brandon\" + 0.051*\"creditors\" + 0.039*\"nearly\" + '\n",
      "  '0.036*\"corrections_appeared\" + 0.032*\"next\" + 0.029*\"explains\" + '\n",
      "  '0.027*\"national\" + 0.024*\"largest\" + 0.022*\"selected\"'),\n",
      " (1,\n",
      "  '0.085*\"bakhmut\" + 0.083*\"competition\" + 0.081*\"sense\" + 0.075*\"early\" + '\n",
      "  '0.058*\"things\" + 0.037*\"beliefs\" + 0.030*\"island\" + 0.030*\"erdogan\" + '\n",
      "  '0.023*\"fossil_fuels\" + 0.023*\"discovered\"'),\n",
      " (2,\n",
      "  '0.217*\"memoir\" + 0.096*\"movement\" + 0.086*\"unions\" + 0.044*\"career\" + '\n",
      "  '0.030*\"others\" + 0.029*\"turkey\" + 0.027*\"maddening\" + 0.025*\"boeing\" + '\n",
      "  '0.020*\"threadgill\" + 0.017*\"protest\"'),\n",
      " (3,\n",
      "  '0.160*\"vote\" + 0.066*\"system\" + 0.057*\"rockets\" + 0.053*\"spur\" + '\n",
      "  '0.052*\"youth\" + 0.049*\"similarities\" + 0.043*\"russia\" + 0.037*\"thought\" + '\n",
      "  '0.033*\"quickly\" + 0.029*\"unfolds\"'),\n",
      " (4,\n",
      "  '0.085*\"vice\" + 0.078*\"expressions\" + 0.055*\"gang\" + 0.052*\"happening\" + '\n",
      "  '0.045*\"hotels\" + 0.039*\"giants\" + 0.037*\"balm\" + 0.034*\"sherpas\" + '\n",
      "  '0.029*\"regulators\" + 0.024*\"four\"'),\n",
      " (5,\n",
      "  '0.107*\"get\" + 0.101*\"misconduct\" + 0.091*\"digital\" + 0.071*\"struggling\" + '\n",
      "  '0.054*\"seemingly\" + 0.044*\"riviera\" + 0.037*\"acts\" + 0.027*\"bottom\" + '\n",
      "  '0.020*\"park\" + 0.018*\"bazaar\"'),\n",
      " (6,\n",
      "  '0.090*\"real\" + 0.075*\"emigration\" + 0.058*\"part\" + 0.047*\"ban\" + '\n",
      "  '0.044*\"debut_novel\" + 0.041*\"bill\" + 0.038*\"headed\" + 0.037*\"struggled\" + '\n",
      "  '0.033*\"runoff\" + 0.024*\"commits\"'),\n",
      " (7,\n",
      "  '0.455*\"story\" + 0.055*\"parking\" + 0.052*\"african\" + 0.037*\"records\" + '\n",
      "  '0.031*\"traumatic\" + 0.021*\"undercut\" + 0.013*\"state\" + 0.013*\"objections\" + '\n",
      "  '0.010*\"grounds\" + 0.008*\"raised_questions\"'),\n",
      " (8,\n",
      "  '0.052*\"interviews\" + 0.030*\"failure\" + 0.021*\"fatal\" + 0.013*\"answers\" + '\n",
      "  '0.012*\"lower\" + 0.000*\"officer\" + 0.000*\"researchers\" + 0.000*\"conference\" '\n",
      "  '+ 0.000*\"college\" + 0.000*\"warning\"'),\n",
      " (9,\n",
      "  '0.167*\"benefiting\" + 0.083*\"willa\" + 0.071*\"tom\" + 0.064*\"find\" + '\n",
      "  '0.052*\"says\" + 0.048*\"days\" + 0.044*\"gains\" + 0.030*\"shoes\" + '\n",
      "  '0.028*\"exporter\" + 0.027*\"diplomatic\"'),\n",
      " (10,\n",
      "  '0.095*\"recep\" + 0.089*\"held\" + 0.075*\"looking\" + 0.053*\"drama\" + '\n",
      "  '0.052*\"former\" + 0.049*\"brings\" + 0.041*\"million\" + 0.037*\"round\" + '\n",
      "  '0.033*\"continues\" + 0.032*\"longtime\"'),\n",
      " (11,\n",
      "  '0.100*\"examination\" + 0.091*\"turtleneck\" + 0.067*\"center\" + 0.066*\"judaism\" '\n",
      "  '+ 0.058*\"eastern\" + 0.047*\"40s\" + 0.046*\"sweet\" + 0.031*\"tries\" + '\n",
      "  '0.030*\"style\" + 0.027*\"home\"'),\n",
      " (12,\n",
      "  '0.268*\"day\" + 0.124*\"quotation\" + 0.103*\"may\" + 0.072*\"tuesday\" + '\n",
      "  '0.064*\"draw\" + 0.050*\"hyped\" + 0.040*\"since\" + 0.021*\"star\" + '\n",
      "  '0.016*\"year_old\" + 0.016*\"basketball\"'),\n",
      " (13,\n",
      "  '0.088*\"invite\" + 0.078*\"protect\" + 0.066*\"emergency\" + 0.051*\"violence\" + '\n",
      "  '0.050*\"covid\" + 0.039*\"latest\" + 0.036*\"roys\" + 0.032*\"usually\" + '\n",
      "  '0.026*\"michael\" + 0.021*\"makes\"'),\n",
      " (14,\n",
      "  '0.076*\"dna\" + 0.067*\"said\" + 0.056*\"nose\" + 0.040*\"instability\" + '\n",
      "  '0.036*\"passed\" + 0.034*\"votes\" + 0.026*\"hit\" + 0.026*\"voted\" + '\n",
      "  '0.025*\"general\" + 0.025*\"members\"'),\n",
      " (15,\n",
      "  '0.084*\"hides\" + 0.063*\"sprung\" + 0.045*\"attacks\" + 0.042*\"dress\" + '\n",
      "  '0.040*\"ability\" + 0.038*\"protagonist\" + 0.036*\"many\" + 0.035*\"exploding\" + '\n",
      "  '0.034*\"sprawling\" + 0.030*\"hopeful\"'),\n",
      " (16,\n",
      "  '0.213*\"cosplays\" + 0.139*\"tragedy\" + 0.095*\"leaders\" + 0.062*\"spanish\" + '\n",
      "  '0.041*\"era\" + 0.038*\"poses\" + 0.021*\"lifted\" + 0.017*\"scientists\" + '\n",
      "  '0.014*\"resilience\" + 0.013*\"bit\"'),\n",
      " (17,\n",
      "  '0.188*\"shoot\" + 0.152*\"parts\" + 0.072*\"airbus\" + 0.052*\"immigrants\" + '\n",
      "  '0.046*\"thousands\" + 0.040*\"california\" + 0.033*\"pastors\" + 0.029*\"data\" + '\n",
      "  '0.022*\"actor\" + 0.022*\"showtime\"'),\n",
      " (18,\n",
      "  '0.083*\"connor\" + 0.079*\"appeared\" + 0.056*\"succession\" + 0.050*\"students\" + '\n",
      "  '0.028*\"bringing\" + 0.027*\"articles\" + 0.027*\"wooed\" + 0.025*\"incite\" + '\n",
      "  '0.025*\"spoilers\" + 0.024*\"nears\"'),\n",
      " (19,\n",
      "  '0.057*\"japanese\" + 0.050*\"poker\" + 0.041*\"recent\" + 0.038*\"principals\" + '\n",
      "  '0.036*\"urged\" + 0.036*\"citizens\" + 0.036*\"saloons\" + 0.034*\"june\" + '\n",
      "  '0.033*\"vaudine\" + 0.030*\"social_media\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the keyword in the 10 topics\n",
    "from pprint import pprint\n",
    "pprint(lda_model.print_topics(num_topics=20, num_words=10))\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
